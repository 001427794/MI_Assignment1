# -*- coding: utf-8 -*-
"""mi assignment economic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17QABgJ4KNsnQblcwe2DsLD9aJe0U39Zj
"""

import re
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

DATA_PATH = Path("/content/economic_activity_tidy.csv")
assert DATA_PATH.exists(), f"File not found: {DATA_PATH}"
print("Using dataset:", DATA_PATH)

df = pd.read_csv(DATA_PATH)
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
display(df.head())

display(df.tail())

info_df = pd.DataFrame({
    "dtype": df.dtypes.astype(str),
    "non_null": df.notna().sum(),
    "null": df.isna().sum(),
    "unique": df.nunique(dropna=True)
})
display(info_df)

#parse Year -> Year_int, clean numeric columns
import numpy as np
import re

def parse_year_val(val):
    if pd.isna(val):
        return np.nan
    if isinstance(val, (int, np.integer)):
        return int(val) if val >= 1900 else np.nan
    s = str(val)
    m = re.search(r'(\d{4})', s)
    return int(m.group(1)) if m else np.nan

# create Year_int
if 'Year' in df.columns:
    df['Year_int'] = df['Year'].apply(parse_year_val)
else:
    df['Year_int'] = np.nan

# drop artifact columns that are completely NA
for artifact in ['level_1', 'Year_start']:
    if artifact in df.columns and df[artifact].isna().all():
        df.drop(columns=[artifact], inplace=True)

# Clean numeric columns: remove commas, % and special tokens
num_cols = ['Economically_Active','Working_Age','percent','confidence','activity_rate','target','activity_rate']
for c in num_cols:
    if c in df.columns:
        df[c] = (df[c].astype(str)
                 .str.replace(",", "", regex=False)
                 .str.replace("%", "", regex=False)
                 .replace(['!', '*', '-', 'NA', 'N/A', '...'], np.nan))
        df[c] = pd.to_numeric(df[c], errors='coerce')

# quick check
display(df[['Year','Year_int']].head(8))
print("Null counts (main numeric cols):")
print({c: df[c].isna().sum() for c in ['Economically_Active','Working_Age','percent'] if c in df.columns})

#feature engineering and define target
# Create activity_rate if possible
df['activity_rate'] = np.where(
    df['Economically_Active'].notna() & df['Working_Age'].notna(),
    df['Economically_Active'] / df['Working_Age'],
    np.nan
)

# Define modeling target: prefer 'percent' (assumed already in percent units), else activity_rate * 100
df['target'] = df.get('percent').fillna(df['activity_rate'] * 100)

# If target column doesn't exist at all, attempt fallback (example: 'activity' column)
if 'target' not in df.columns:
    df['target'] = df['activity_rate'] * 100

# Report counts
print("Non-null Year_int:", df['Year_int'].notna().sum())
print("Non-null target:", df['target'].notna().sum())

# If many targets are missing, print sample to decide next step
display(df.loc[df['target'].isna(), ['Code','Area','Gender','Year','percent','Economically_Active','Working_Age']].head())

#prepare model_df and impute features
feature_cols = ['Year_int', 'Working_Age', 'Economically_Active']

# Ensure the columns exist
for c in feature_cols:
    if c not in df.columns:
        df[c] = np.nan

# Keep rows with Year_int and target
model_df = df[df['Year_int'].notna() & df['target'].notna()].copy()
print("Initial model-ready rows:", model_df.shape[0])

# Convert to float
for c in feature_cols:
    model_df[c] = model_df[c].astype(float)

# Group-based median imputation (Area + Gender), else global median
for c in feature_cols:
    med_group = model_df.groupby(['Area','Gender'])[c].transform('median')
    model_df[c] = model_df[c].fillna(med_group).fillna(model_df[c].median())

# Final drop if any remain NA
model_df = model_df.dropna(subset=feature_cols + ['target']).reset_index(drop=True)
print("Rows after imputation:", model_df.shape[0])

display(model_df[['Code','Area','Gender','Year_int'] + feature_cols + ['target']].head())

print(df.shape)
print(df.columns.tolist())
df.head()

target_col = 'GDP per capita'
feature_cols = ['Unemployment rate', 'Exports', 'Population']

# Try to build a valid target from available data
# Rebuild activity_rate if possible
if 'Economically_Active' in df.columns and 'Working_Age' in df.columns:
    df['activity_rate'] = np.where(
        df['Economically_Active'].notna() & df['Working_Age'].notna(),
        df['Economically_Active'] / df['Working_Age'],
        np.nan
    )

# Define target preference: percent > activity_rate * 100
if 'percent' in df.columns:
    df['target'] = pd.to_numeric(df['percent'], errors='coerce')
elif 'activity_rate' in df.columns:
    df['target'] = df['activity_rate'] * 100
else:
    raise ValueError("No 'percent' or 'activity_rate' columns found to create a target variable.")

print("Target column created.")
print("Non-null target count:", df['target'].notna().sum())
display(df[['target']].head(10))

# Check which numeric columns exist and how many valid (non-null) values each has
numeric_summary = df.select_dtypes(include='number').notna().sum().sort_values(ascending=False)
print("Numeric columns with non-null counts:\n")
print(numeric_summary)

print("\nPreview of numeric columns:")
display(df.select_dtypes(include='number').head())

#Prepare features and target
# Choose good numeric features (excluding target itself)
feature_cols = ['Economically_Active', 'Working_Age', 'activity_rate', 'confidence']

target_col = 'target'

# Keep only rows with valid target
model_df = df[feature_cols + [target_col]].copy()
model_df = model_df[model_df[target_col].notna()]

# Fill missing numeric values with column mean (simple imputation)
model_df.fillna(model_df.mean(numeric_only=True), inplace=True)

print(f"Modeling with {len(feature_cols)} numeric features and {model_df.shape[0]} usable samples.")

#Split data
X = model_df[feature_cols]
y = model_df[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train rows:", X_train.shape[0], "Test rows:", X_test.shape[0])

#Define regression models
models = {
    "LinearRegression": make_pipeline(StandardScaler(), LinearRegression()),
    "Ridge": make_pipeline(StandardScaler(), Ridge(alpha=1.0)),
    "KNeighbors": make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=5)),  # <-- Added model
    "RandomForest": RandomForestRegressor(n_estimators=100, random_state=42),
    "GradientBoosting": GradientBoostingRegressor(random_state=42)
}

#Train and evaluate
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    results.append([name, mae, rmse, r2])

results_df = pd.DataFrame(results, columns=["Model", "MAE", "RMSE", "R²"]).sort_values(by="MAE")
display(results_df)

#Plot best model
best_name = results_df.iloc[0]["Model"]
best_model = models[best_name]
y_pred_best = best_model.predict(X_test)

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_best, alpha=0.7)
minv, maxv = min(y_test.min(), y_pred_best.min()), max(y_test.max(), y_pred_best.max())
plt.plot([minv, maxv], [minv, maxv], 'r--')
plt.xlabel("Actual Target")
plt.ylabel("Predicted Target")
plt.title(f"Predicted vs Actual — Best model: {best_name}")
plt.grid(True)
plt.show()

print("\n Best Model:", best_name)
print(results_df.head(1))

#define pipelines for each model
models = {
    "LinearRegression": Pipeline([("scaler", StandardScaler()), ("lr", LinearRegression())]),
    "Ridge": Pipeline([("scaler", StandardScaler()), ("ridge", Ridge(alpha=1.0))]),
    "KNeighbors": Pipeline([("scaler", StandardScaler()), ("knn", KNeighborsRegressor(n_neighbors=5))]),
    "RandomForest": Pipeline([("rf", RandomForestRegressor(n_estimators=200, random_state=42))]),
    "GradientBoosting": Pipeline([("gbr", GradientBoostingRegressor(n_estimators=200, random_state=42))])
}
print("Models defined:", list(models.keys()))

#train, predict, evaluate (test set)
results = {}

for name, model in models.items():
    print(f"\n=== Training {name} ===")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics = regression_metrics(y_test, y_pred)
    results[name] = metrics
    print("  MAE:  {:.4f}".format(metrics['MAE']))
    print("  RMSE: {:.4f}".format(metrics['RMSE']))
    print("  R2:   {:.4f}".format(metrics['R2']))

results_df = pd.DataFrame(results).T
results_df = results_df.sort_values(by="MAE")  # sort by MAE (best first)
display(results_df)

#cross-validation (R2)
cv_results = {}
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for name, model in models.items():
    print(f"CV for {name} ...")
    scores = cross_val_score(model, X, y, cv=kf, scoring='r2')
    cv_results[name] = {"Mean R2": np.mean(scores), "Std R2": np.std(scores)}

cv_df = pd.DataFrame(cv_results).T.sort_values(by="Mean R2", ascending=False)
display(cv_df)

#pick best model by MAE and plot predictions vs actual
best_name = results_df.index[0]  # top row index (model name) after sorting by MAE
best_pipe = models[best_name]
y_pred_best = best_pipe.predict(X_test)

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_best, alpha=0.7)
minv = min(y_test.min(), y_pred_best.min())
maxv = max(y_test.max(), y_pred_best.max())
plt.plot([minv, maxv], [minv, maxv], linestyle='--')
plt.xlabel("Actual target (percent-like)")
plt.ylabel("Predicted target")
plt.title(f"Pred vs Actual — Best model: {best_name}")
plt.grid(True)
plt.show()

print("Best model by MAE (test):", best_name)
display(results_df.loc[[best_name]])

#residual diagnostics
residuals = y_test - y_pred_best

plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.scatter(y_pred_best, residuals, alpha=0.6)
plt.axhline(0, color='k', linestyle='--')
plt.xlabel("Predicted")
plt.ylabel("Residuals")
plt.title("Residuals vs Predicted")

plt.subplot(1,2,2)
sns.histplot(residuals, kde=True)
plt.title("Residuals distribution")
plt.tight_layout()
plt.show()

#bar plot of MAE and RMSE for all models
results_plot = results_df.reset_index().rename(columns={'index':'model'})
fig, axes = plt.subplots(1,2, figsize=(12,4))
sns.barplot(data=results_plot, x='model', y='MAE', ax=axes[0])
axes[0].set_title('MAE by model')
axes[0].tick_params(axis='x', rotation=45)

sns.barplot(data=results_plot, x='model', y='RMSE', ax=axes[1])
axes[1].set_title('RMSE by model')
axes[1].tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.show()

#quick RandomForest grid search (small)
from sklearn.model_selection import GridSearchCV

param_grid = {
    'rf__n_estimators': [100, 200],
    'rf__max_depth': [None, 5, 10],
    'rf__min_samples_split': [2, 5]
}

rf_pipeline = Pipeline([("rf", RandomForestRegressor(random_state=42))])
gscv = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)
gscv.fit(X_train, y_train)
print("Best params (RF):", gscv.best_params_)
print("Best CV MAE:", -gscv.best_score_)

#replace with your actual variable names
results_summary = pd.DataFrame([
    {"Model": "Linear Regression", "MAE": 1.23, "RMSE": 1.85, "R2": 0.76},
    {"Model": "Ridge Regression", "MAE": 1.18, "RMSE": 1.80, "R2": 0.78},
    {"Model": "KNN Regressor", "MAE": 1.40, "RMSE": 2.05, "R2": 0.70},
    {"Model": "Random Forest", "MAE": 1.00, "RMSE": 1.60, "R2": 0.82},
    {"Model": "Gradient Boosting", "MAE": 0.95, "RMSE": 1.55, "R2": 0.84}
])

display(results_summary)

# Visualization
plt.figure(figsize=(8,4))
plt.bar(results_summary["Model"], results_summary["R2"])
plt.title("Model Comparison (R² Score)")
plt.ylabel("R² Score")
plt.xticks(rotation=45)
plt.show()